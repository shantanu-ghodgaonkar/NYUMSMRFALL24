{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5q019WzMswH"
      },
      "source": [
        "# Q2 Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Let us import necessary modules"
      ],
      "metadata": {
        "id": "bHPSynb6qRG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jSHs2QReMqVn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Display version of SDKs and setup device agnostic code"
      ],
      "metadata": {
        "id": "HvamJAKHqdGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the versions of PyTorch and torchvision being used\n",
        "print(\n",
        "    f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\"\n",
        ")\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Setup device-agnostic code to use GPU if available, else CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    # Use CUDA tensors by default\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    # Use CPU tensors by default\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxpbUZ4SqWjK",
        "outputId": "02a94f8a-50d5-46ee-ca83-8b159107f77e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.4.1+cu121\n",
            "torchvision version: 0.19.1+cu121\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the dataset and set the batch size"
      ],
      "metadata": {
        "id": "X8b7X-WKqk38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data using FashionMNIST dataset\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Setup testing data using FashionMNIST dataset\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Define batch size for DataLoader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoader for training data with shuffling and appropriate generator for CUDA\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    generator=torch.Generator(device=\"cuda\") if device == \"cuda\" else None,\n",
        ")\n",
        "\n",
        "# Create DataLoader for testing data without shuffling\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    generator=torch.Generator(device=\"cuda\") if device == \"cuda\" else None,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4JQmnPlqqQ0",
        "outputId": "c326513f-b8e2-4fd6-d75c-fcce853a4cac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:08<00:00, 3209962.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 199142.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3715572.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6745478.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create the model class"
      ],
      "metadata": {
        "id": "WTEp0q5Eqrw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Variational Autoencoder (VAE) model for the FashionMNIST dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=784, hidden_units=512, output_shape=128):\n",
        "        \"\"\"\n",
        "        Initializes the VAE with encoder and decoder networks.\n",
        "\n",
        "        Args:\n",
        "            input_shape (int): Size of the input layer (default: 784 for 28x28 images).\n",
        "            hidden_units (int): Number of hidden units in the encoder (default: 512).\n",
        "            output_shape (int): Size of the latent space (default: 128).\n",
        "        \"\"\"\n",
        "        super(FashionMNISTVAE, self).__init__()\n",
        "\n",
        "        # Ensure that input_shape, hidden_units, and output_shape are multiples of 2\n",
        "        if ((input_shape % 2 != 0) | (hidden_units % 2 != 0) | (output_shape % 2 != 0)):\n",
        "            raise ValueError(\"Ensure that all parameters are multiples of 2\")\n",
        "\n",
        "        # Define the encoder network\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the input image\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units,\n",
        "                      out_features=int(hidden_units / 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=int(hidden_units / 2),\n",
        "                      out_features=output_shape),\n",
        "        )\n",
        "\n",
        "        # Linear layers to compute mean and log variance of the latent space\n",
        "        self.fc_mu = nn.Linear(output_shape, int(\n",
        "            output_shape / 4))      # Mean vector\n",
        "        self.fc_logvar = nn.Linear(output_shape, int(\n",
        "            output_shape / 4))  # Log variance\n",
        "\n",
        "        # Define the decoder network\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=int(output_shape / 4),\n",
        "                      out_features=int(hidden_units / 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=int(hidden_units / 2),\n",
        "                      out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=input_shape),\n",
        "            nn.Sigmoid(),  # Sigmoid activation for output to be between 0 and 1\n",
        "        )\n",
        "\n",
        "    def encoder_func(self, x):\n",
        "        \"\"\"\n",
        "        Passes input through the encoder and returns mean and log variance.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: Mean and log variance tensors.\n",
        "        \"\"\"\n",
        "        x = self.encoder(x)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)\n",
        "\n",
        "    def reparametrization_trick(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        Applies the reparametrization trick to sample from the latent space.\n",
        "\n",
        "        Args:\n",
        "            mu (Tensor): Mean tensor.\n",
        "            logvar (Tensor): Log variance tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Sampled latent vector.\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)  # Calculate standard deviation\n",
        "        # Sample epsilon from a standard normal distribution\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std          # Return the sampled latent vector\n",
        "\n",
        "    def decoder_func(self, z):\n",
        "        \"\"\"\n",
        "        Passes latent vector through the decoder to reconstruct the input.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): Latent vector.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reconstructed input.\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the VAE.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor, Tensor]: Reconstructed input, mean, and log variance.\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encoder_func(\n",
        "            x)          # Encode input to get mean and log variance\n",
        "        # Sample latent vector using reparametrization trick\n",
        "        z = self.reparametrization_trick(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar         # Decode the latent vector\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A-muDX3Pq2Yb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create the loss function"
      ],
      "metadata": {
        "id": "U0AUctHrq6pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FashionMNISTVAE_loss_func(org, recon, mu, logvar):\n",
        "    \"\"\"\n",
        "    Computes the VAE loss function as the sum of reconstruction loss and KL divergence.\n",
        "\n",
        "    Args:\n",
        "        org (Tensor): Original input tensor.\n",
        "        recon (Tensor): Reconstructed input tensor.\n",
        "        mu (Tensor): Mean tensor from the encoder.\n",
        "        logvar (Tensor): Log variance tensor from the encoder.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Total loss.\n",
        "    \"\"\"\n",
        "    # Reconstruction loss using binary cross entropy\n",
        "    reconstruction_loss = nn.functional.binary_cross_entropy(\n",
        "        recon.view(-1, 28 * 28), org.view(-1, 28 * 28), reduction='sum'\n",
        "    )\n",
        "\n",
        "    # KL divergence loss to regularize the latent space\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - (mu ** 2) - torch.exp(logvar))\n",
        "\n",
        "    return reconstruction_loss + kl_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "wXbFKwYSruqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create a function to plot decoded images"
      ],
      "metadata": {
        "id": "aeg7Gaejq-_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reconstructed_images(model, n_images=10):\n",
        "    \"\"\"\n",
        "    Generates and plots reconstructed images from the VAE's decoder.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained VAE model.\n",
        "        n_images (int): Number of images to generate and plot (default: 10).\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        # Generate a latent vector `z` with the size expected by the decoder\n",
        "        z = torch.randn(n_images, 4)  # 4 is the size expected by the decoder\n",
        "\n",
        "        # Pass the latent vector through the decoder to generate images\n",
        "        generated = model.decoder_func(z)\n",
        "\n",
        "        # Reshape the generated output to be 28x28 images with 1 channel\n",
        "        generated = generated.view(-1, 1, 28, 28)\n",
        "\n",
        "        # Move the generated images to the CPU if using CUDA\n",
        "        if model.decoder[0].weight.is_cuda:\n",
        "            generated = generated.to('cpu')\n",
        "\n",
        "    # Plot the generated images using matplotlib\n",
        "    fig, axes = plt.subplots(1, n_images, figsize=(n_images, 1))\n",
        "    for i in range(n_images):\n",
        "        # Display each image in grayscale\n",
        "        axes[i].imshow(generated[i].squeeze(), cmap='gray')\n",
        "        axes[i].axis('off')  # Hide the axis\n",
        "    plt.waitforbuttonpress()  # Wait for a button press to close the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "igrr1Evzr0E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Create a a function to train and save the model"
      ],
      "metadata": {
        "id": "gFb_UmoirGBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_save_model(epochs=50, input_shape=784, hidden_units=256, output_shape=16, lr=0.001, MODEL_PATH=Path(\"./Q2/saved_models\").absolute()):\n",
        "    \"\"\"\n",
        "    Trains the VAE model and saves the trained model's state dictionary.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of training epochs (default: 50).\n",
        "        input_shape (int): Size of the input layer (default: 784 for 28x28 images).\n",
        "        hidden_units (int): Number of hidden units in the encoder (default: 256).\n",
        "        output_shape (int): Size of the latent space (default: 16).\n",
        "        lr (float): Learning rate for the optimizer (default: 0.001).\n",
        "        MODEL_PATH (Path): Folder to store the trained model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the model save path with a timestamp\n",
        "    MODEL_NAME = f\"FashionMNISTVAE_model_{int(time())}.pth\"\n",
        "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(\n",
        "        True)  # Enable anomaly detection for debugging\n",
        "    model = FashionMNISTVAE(\n",
        "        hidden_units=hidden_units, output_shape=output_shape\n",
        "    ).to(device=device)  # Initialize and move the model to the appropriate device\n",
        "\n",
        "    # Initialize the Adam optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    start_time = time()  # Record the start time\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time()  # Record the start time of the epoch\n",
        "        model.train()  # Set the model to training mode\n",
        "        tot_loss = 0\n",
        "        tot_reconstruction_loss = 0\n",
        "        tot_kl_loss = 0\n",
        "\n",
        "        for data, _ in train_dataloader:\n",
        "            data = data.to(device)  # Move data to the appropriate device\n",
        "            optimizer.zero_grad()   # Reset gradients\n",
        "\n",
        "            recon, mu, logvar = model(data)  # Forward pass through the model\n",
        "\n",
        "            # Calculate reconstruction loss using binary cross entropy\n",
        "            reconstruction_loss = nn.functional.binary_cross_entropy(\n",
        "                recon.view(-1, 28 * 28), data.view(-1, 28 * 28), reduction='sum'\n",
        "            )\n",
        "\n",
        "            # Calculate KL divergence loss\n",
        "            kl_loss = -0.5 * \\\n",
        "                torch.sum(1 + logvar - (mu ** 2) - torch.exp(logvar))\n",
        "\n",
        "            # Total loss is the sum of reconstruction loss and KL divergence\n",
        "            loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            # Accumulate losses for monitoring\n",
        "            tot_loss += loss.item()\n",
        "            tot_reconstruction_loss += reconstruction_loss.item()\n",
        "            tot_kl_loss += kl_loss.item()\n",
        "\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(\n",
        "            f'Epoch [{epoch+1}/{epochs}], '\n",
        "            f'Total Loss: {tot_loss / len(train_dataloader.dataset):.4f}, '\n",
        "            f'Reconstruction Loss: {tot_reconstruction_loss / len(train_dataloader.dataset):.4f}, '\n",
        "            f'KL Divergence Loss: {tot_kl_loss / len(train_dataloader.dataset):.4f}, '\n",
        "            f'Time taken: {time() - epoch_start_time:.2f}s'\n",
        "        )\n",
        "\n",
        "    # Print total training time\n",
        "    print(f\"Time taken for {epochs} epochs is {time() - start_time:.2f}s\")\n",
        "\n",
        "    # Save the trained model's state dictionary\n",
        "    print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "    torch.save(\n",
        "        obj=model.state_dict(),  # Save only the model's learned parameters\n",
        "        f=MODEL_SAVE_PATH\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "t8ZyE0cJr26c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Create a function to load the saved model"
      ],
      "metadata": {
        "id": "Gg6e99IurWal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_model(path, input_shape=784, hidden_units=256, output_shape=16):\n",
        "    \"\"\"\n",
        "    Loads a saved VAE model from the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str or Path): Path to the saved model's state dictionary.\n",
        "        input_shape (int): Size of the input layer (default: 784 for 28x28 images).\n",
        "        hidden_units (int): Number of hidden units in the encoder (default: 256).\n",
        "        output_shape (int): Size of the latent space (default: 16).\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Loaded VAE model.\n",
        "    \"\"\"\n",
        "    loaded_model = FashionMNISTVAE(\n",
        "        hidden_units=hidden_units, output_shape=output_shape\n",
        "    )\n",
        "    loaded_model.load_state_dict(torch.load(path))  # Load the state dictionary\n",
        "    loaded_model = loaded_model.to(\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )  # Move the model to the appropriate device\n",
        "    return loaded_model\n",
        "\n"
      ],
      "metadata": {
        "id": "z2vv2SXSr3o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Main Code"
      ],
      "metadata": {
        "id": "q_NV7f-_rb73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the line below to train and save the model\n",
        "# train_save_model(epochs=50)\n",
        "\n",
        "# Path to the saved model (update the path as needed)\n",
        "saved_model_path = Path(\n",
        "\"./Q2/saved_models/FashionMNISTVAE_model_1728094341.pth\").absolute()\n",
        "\n",
        "# Load the saved model\n",
        "model = load_saved_model(saved_model_path)\n",
        "\n",
        "# Plot reconstructed images using the loaded model\n",
        "plot_reconstructed_images(model=model)"
      ],
      "metadata": {
        "id": "Yn7KblAxrfxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}